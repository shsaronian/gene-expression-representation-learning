{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked AE GSE99095.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA4ICQL9L31H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d5d7c3bf-42b8-4187-b721-823679619256"
      },
      "source": [
        "#import necessary packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2hPKKfdOg_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e0387557-c752-438b-a53a-6fb61bcd3b02"
      },
      "source": [
        "#read data and split labels from values\n",
        "initial = (pd.read_csv('/content/drive/My Drive/GSE99095_normalizedExpression.csv',header=None)).T\n",
        "#create labels\n",
        "initial[0]=np.zeros(980)\n",
        "for i in range(1,392):\n",
        "    (initial[0].values)[i]=1\n",
        "\n",
        "initial=initial.iloc[1:]\n",
        "labels= initial[0].values\n",
        "values = (initial.drop((initial[0]),axis=1)).values"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4yyb21wOlPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#randomly shuffle data and do a train-test split on values and labels\n",
        "values, labels = shuffle(values, labels,random_state=1) \n",
        "values_train = values[:700, :]\n",
        "values_test = values[700:, :]\n",
        "y_train = labels[:700]\n",
        "y_test = labels[700:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfwPCQI5PGPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee01fcbb-a2ca-4afc-cfa1-ba828ba22f18"
      },
      "source": [
        "#build the autoencoder\n",
        "inputs = keras.Input(values.shape[1],)\n",
        "b1 = layers.BatchNormalization(name=\"batch1\")(inputs)\n",
        "lr1 = layers.Dense(8192, activation=\"softplus\", name=\"lr1\")(b1)\n",
        "b2 = layers.BatchNormalization(name=\"batch2\")(lr1)\n",
        "lr2 = layers.Dense(1024, activation=\"softplus\", name=\"lr2\")(b2)\n",
        "b3 = layers.BatchNormalization(name=\"batch3\")(lr2)\n",
        "lr3 = layers.Dense(128, activation=\"softplus\", name=\"lr3\")(b3)\n",
        "b4 = layers.BatchNormalization(name=\"batch4\")(lr3)\n",
        "lr4 = layers.Dense(2048, activation=\"softplus\", name=\"lr4\")(b4)\n",
        "b5 = layers.BatchNormalization(name=\"batch5\")(lr4)\n",
        "outputs = layers.Dense(values.shape[1], activation=\"softplus\", name=\"outputs\")(b5)\n",
        "\n",
        "#build a seperate encoder for feature extraction\n",
        "encoder = keras.models.Model(inputs, lr3)\n",
        "\n",
        "#connect the autoencoder\n",
        "stacked_ae = keras.models.Model(inputs,outputs)\n",
        "\n",
        "#compile the model\n",
        "stacked_ae.compile(loss=\"mean_squared_error\",optimizer=keras.optimizers.Nadam(learning_rate=0.0001))\n",
        "\n",
        "#train the model\n",
        "h_stack = stacked_ae.fit(values_train, values_train, epochs=200,batch_size=8,validation_data=[values_test, values_test])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 700 samples, validate on 279 samples\n",
            "Epoch 1/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.5821 - val_loss: 2.5691\n",
            "Epoch 2/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.5328 - val_loss: 2.5206\n",
            "Epoch 3/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.4983 - val_loss: 2.4748\n",
            "Epoch 4/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.4530 - val_loss: 2.4188\n",
            "Epoch 5/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.3884 - val_loss: 2.3608\n",
            "Epoch 6/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.3016 - val_loss: 2.2910\n",
            "Epoch 7/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.2208 - val_loss: 2.2350\n",
            "Epoch 8/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.1532 - val_loss: 2.1770\n",
            "Epoch 9/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.1002 - val_loss: 2.1440\n",
            "Epoch 10/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.0567 - val_loss: 2.1104\n",
            "Epoch 11/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 2.0205 - val_loss: 2.0999\n",
            "Epoch 12/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.9978 - val_loss: 2.0894\n",
            "Epoch 13/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.9752 - val_loss: 2.0822\n",
            "Epoch 14/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.9533 - val_loss: 2.0718\n",
            "Epoch 15/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.9338 - val_loss: 2.0644\n",
            "Epoch 16/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.9175 - val_loss: 2.0612\n",
            "Epoch 17/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.9012 - val_loss: 2.0604\n",
            "Epoch 18/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.8836 - val_loss: 2.0592\n",
            "Epoch 19/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.8678 - val_loss: 2.0591\n",
            "Epoch 20/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.8528 - val_loss: 2.0554\n",
            "Epoch 21/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.8368 - val_loss: 2.0563\n",
            "Epoch 22/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.8246 - val_loss: 2.0562\n",
            "Epoch 23/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.8080 - val_loss: 2.0537\n",
            "Epoch 24/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7941 - val_loss: 2.0513\n",
            "Epoch 25/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7811 - val_loss: 2.0521\n",
            "Epoch 26/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7700 - val_loss: 2.0503\n",
            "Epoch 27/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7540 - val_loss: 2.0523\n",
            "Epoch 28/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7414 - val_loss: 2.0519\n",
            "Epoch 29/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7305 - val_loss: 2.0521\n",
            "Epoch 30/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7218 - val_loss: 2.0544\n",
            "Epoch 31/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.7087 - val_loss: 2.0508\n",
            "Epoch 32/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6998 - val_loss: 2.0518\n",
            "Epoch 33/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6921 - val_loss: 2.0478\n",
            "Epoch 34/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6791 - val_loss: 2.0554\n",
            "Epoch 35/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6715 - val_loss: 2.0463\n",
            "Epoch 36/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6694 - val_loss: 2.0442\n",
            "Epoch 37/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6527 - val_loss: 2.0469\n",
            "Epoch 38/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6456 - val_loss: 2.0465\n",
            "Epoch 39/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6359 - val_loss: 2.0470\n",
            "Epoch 40/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6321 - val_loss: 2.0463\n",
            "Epoch 41/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6224 - val_loss: 2.0455\n",
            "Epoch 42/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6158 - val_loss: 2.0451\n",
            "Epoch 43/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6113 - val_loss: 2.0455\n",
            "Epoch 44/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.6032 - val_loss: 2.0463\n",
            "Epoch 45/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5951 - val_loss: 2.0440\n",
            "Epoch 46/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5907 - val_loss: 2.0457\n",
            "Epoch 47/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5861 - val_loss: 2.0467\n",
            "Epoch 48/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5826 - val_loss: 2.0443\n",
            "Epoch 49/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5753 - val_loss: 2.0448\n",
            "Epoch 50/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5715 - val_loss: 2.0443\n",
            "Epoch 51/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5639 - val_loss: 2.0460\n",
            "Epoch 52/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5654 - val_loss: 2.0459\n",
            "Epoch 53/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5600 - val_loss: 2.0474\n",
            "Epoch 54/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5484 - val_loss: 2.0486\n",
            "Epoch 55/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5454 - val_loss: 2.0476\n",
            "Epoch 56/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5480 - val_loss: 2.0447\n",
            "Epoch 57/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5383 - val_loss: 2.0446\n",
            "Epoch 58/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5393 - val_loss: 2.0483\n",
            "Epoch 59/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5259 - val_loss: 2.0470\n",
            "Epoch 60/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5283 - val_loss: 2.0445\n",
            "Epoch 61/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5166 - val_loss: 2.0474\n",
            "Epoch 62/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5253 - val_loss: 2.0452\n",
            "Epoch 63/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5201 - val_loss: 2.0487\n",
            "Epoch 64/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5167 - val_loss: 2.0511\n",
            "Epoch 65/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5134 - val_loss: 2.0494\n",
            "Epoch 66/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5115 - val_loss: 2.0485\n",
            "Epoch 67/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5063 - val_loss: 2.0504\n",
            "Epoch 68/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5037 - val_loss: 2.0522\n",
            "Epoch 69/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.5047 - val_loss: 2.0521\n",
            "Epoch 70/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4987 - val_loss: 2.0543\n",
            "Epoch 71/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4988 - val_loss: 2.0598\n",
            "Epoch 72/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4934 - val_loss: 2.0511\n",
            "Epoch 73/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4953 - val_loss: 2.0540\n",
            "Epoch 74/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4875 - val_loss: 2.0573\n",
            "Epoch 75/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4919 - val_loss: 2.0570\n",
            "Epoch 76/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4883 - val_loss: 2.0596\n",
            "Epoch 77/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4807 - val_loss: 2.0570\n",
            "Epoch 78/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4819 - val_loss: 2.0594\n",
            "Epoch 79/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4768 - val_loss: 2.0600\n",
            "Epoch 80/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4746 - val_loss: 2.0644\n",
            "Epoch 81/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4763 - val_loss: 2.0571\n",
            "Epoch 82/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4765 - val_loss: 2.0595\n",
            "Epoch 83/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4773 - val_loss: 2.0528\n",
            "Epoch 84/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4691 - val_loss: 2.0558\n",
            "Epoch 85/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4751 - val_loss: 2.0560\n",
            "Epoch 86/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4709 - val_loss: 2.0641\n",
            "Epoch 87/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4660 - val_loss: 2.0623\n",
            "Epoch 88/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4678 - val_loss: 2.0540\n",
            "Epoch 89/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4549 - val_loss: 2.0653\n",
            "Epoch 90/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4572 - val_loss: 2.0661\n",
            "Epoch 91/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4627 - val_loss: 2.0635\n",
            "Epoch 92/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4565 - val_loss: 2.0628\n",
            "Epoch 93/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4586 - val_loss: 2.0578\n",
            "Epoch 94/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4504 - val_loss: 2.0612\n",
            "Epoch 95/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4559 - val_loss: 2.0629\n",
            "Epoch 96/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4533 - val_loss: 2.0642\n",
            "Epoch 97/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4519 - val_loss: 2.0699\n",
            "Epoch 98/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4411 - val_loss: 2.0661\n",
            "Epoch 99/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4408 - val_loss: 2.0661\n",
            "Epoch 100/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4454 - val_loss: 2.0666\n",
            "Epoch 101/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4449 - val_loss: 2.0650\n",
            "Epoch 102/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4538 - val_loss: 2.0751\n",
            "Epoch 103/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4374 - val_loss: 2.0859\n",
            "Epoch 104/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4451 - val_loss: 2.0738\n",
            "Epoch 105/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4398 - val_loss: 2.0725\n",
            "Epoch 106/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4409 - val_loss: 2.0700\n",
            "Epoch 107/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4414 - val_loss: 2.0675\n",
            "Epoch 108/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4365 - val_loss: 2.0735\n",
            "Epoch 109/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4365 - val_loss: 2.0889\n",
            "Epoch 110/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4283 - val_loss: 2.0769\n",
            "Epoch 111/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4260 - val_loss: 2.0855\n",
            "Epoch 112/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4321 - val_loss: 2.0910\n",
            "Epoch 113/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4294 - val_loss: 2.0964\n",
            "Epoch 114/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4208 - val_loss: 2.0930\n",
            "Epoch 115/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4211 - val_loss: 2.0972\n",
            "Epoch 116/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4212 - val_loss: 2.0854\n",
            "Epoch 117/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4254 - val_loss: 2.0908\n",
            "Epoch 118/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4242 - val_loss: 2.0834\n",
            "Epoch 119/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4212 - val_loss: 2.0816\n",
            "Epoch 120/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4229 - val_loss: 2.0862\n",
            "Epoch 121/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4226 - val_loss: 2.0978\n",
            "Epoch 122/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4220 - val_loss: 2.0943\n",
            "Epoch 123/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4162 - val_loss: 2.1023\n",
            "Epoch 124/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4156 - val_loss: 2.1085\n",
            "Epoch 125/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4212 - val_loss: 2.0972\n",
            "Epoch 126/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4169 - val_loss: 2.0881\n",
            "Epoch 127/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4217 - val_loss: 2.0892\n",
            "Epoch 128/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4140 - val_loss: 2.0988\n",
            "Epoch 129/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4108 - val_loss: 2.0954\n",
            "Epoch 130/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4105 - val_loss: 2.1030\n",
            "Epoch 131/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4091 - val_loss: 2.0822\n",
            "Epoch 132/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4100 - val_loss: 2.0978\n",
            "Epoch 133/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4026 - val_loss: 2.1009\n",
            "Epoch 134/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4076 - val_loss: 2.0970\n",
            "Epoch 135/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4032 - val_loss: 2.0979\n",
            "Epoch 136/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3958 - val_loss: 2.1424\n",
            "Epoch 137/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4084 - val_loss: 2.1179\n",
            "Epoch 138/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4020 - val_loss: 2.0949\n",
            "Epoch 139/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4015 - val_loss: 2.1011\n",
            "Epoch 140/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3953 - val_loss: 2.0954\n",
            "Epoch 141/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.4015 - val_loss: 2.0936\n",
            "Epoch 142/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3995 - val_loss: 2.1060\n",
            "Epoch 143/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3965 - val_loss: 2.1217\n",
            "Epoch 144/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3906 - val_loss: 2.1080\n",
            "Epoch 145/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3944 - val_loss: 2.0878\n",
            "Epoch 146/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3926 - val_loss: 2.1166\n",
            "Epoch 147/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3918 - val_loss: 2.1073\n",
            "Epoch 148/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3887 - val_loss: 2.1115\n",
            "Epoch 149/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3862 - val_loss: 2.1123\n",
            "Epoch 150/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3889 - val_loss: 2.1325\n",
            "Epoch 151/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3949 - val_loss: 2.1177\n",
            "Epoch 152/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3858 - val_loss: 2.1037\n",
            "Epoch 153/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3805 - val_loss: 2.1242\n",
            "Epoch 154/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3859 - val_loss: 2.1247\n",
            "Epoch 155/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3919 - val_loss: 2.1200\n",
            "Epoch 156/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3790 - val_loss: 2.1381\n",
            "Epoch 157/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3780 - val_loss: 2.1277\n",
            "Epoch 158/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3774 - val_loss: 2.1267\n",
            "Epoch 159/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3883 - val_loss: 2.1125\n",
            "Epoch 160/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3784 - val_loss: 2.1282\n",
            "Epoch 161/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3689 - val_loss: 2.1143\n",
            "Epoch 162/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3677 - val_loss: 2.1391\n",
            "Epoch 163/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3751 - val_loss: 2.1095\n",
            "Epoch 164/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3689 - val_loss: 2.1259\n",
            "Epoch 165/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3729 - val_loss: 2.1323\n",
            "Epoch 166/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3782 - val_loss: 2.1233\n",
            "Epoch 167/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3756 - val_loss: 2.1308\n",
            "Epoch 168/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3717 - val_loss: 2.1464\n",
            "Epoch 169/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3755 - val_loss: 2.1334\n",
            "Epoch 170/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3655 - val_loss: 2.1186\n",
            "Epoch 171/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3716 - val_loss: 2.1274\n",
            "Epoch 172/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3629 - val_loss: 2.1480\n",
            "Epoch 173/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3686 - val_loss: 2.1326\n",
            "Epoch 174/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3635 - val_loss: 2.1338\n",
            "Epoch 175/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3581 - val_loss: 2.1330\n",
            "Epoch 176/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3566 - val_loss: 2.1470\n",
            "Epoch 177/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3577 - val_loss: 2.1406\n",
            "Epoch 178/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3558 - val_loss: 2.1380\n",
            "Epoch 179/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3604 - val_loss: 2.1529\n",
            "Epoch 180/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3706 - val_loss: 2.1749\n",
            "Epoch 181/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3594 - val_loss: 2.1467\n",
            "Epoch 182/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3544 - val_loss: 2.1690\n",
            "Epoch 183/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3551 - val_loss: 2.1439\n",
            "Epoch 184/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3449 - val_loss: 2.1663\n",
            "Epoch 185/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3460 - val_loss: 2.1695\n",
            "Epoch 186/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3522 - val_loss: 2.1661\n",
            "Epoch 187/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3423 - val_loss: 2.1563\n",
            "Epoch 188/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3389 - val_loss: 2.1656\n",
            "Epoch 189/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3519 - val_loss: 2.1720\n",
            "Epoch 190/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3541 - val_loss: 2.1687\n",
            "Epoch 191/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3370 - val_loss: 2.1595\n",
            "Epoch 192/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3546 - val_loss: 2.1385\n",
            "Epoch 193/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3485 - val_loss: 2.1406\n",
            "Epoch 194/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3458 - val_loss: 2.1302\n",
            "Epoch 195/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3341 - val_loss: 2.1779\n",
            "Epoch 196/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3459 - val_loss: 2.1642\n",
            "Epoch 197/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3443 - val_loss: 2.1568\n",
            "Epoch 198/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3358 - val_loss: 2.1822\n",
            "Epoch 199/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3352 - val_loss: 2.1698\n",
            "Epoch 200/200\n",
            "700/700 [==============================] - 23s 33ms/sample - loss: 1.3435 - val_loss: 2.1534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OybcJQCRien_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract the features for whole data using trained model\n",
        "features= encoder.predict(values)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Al9tHNq22d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "571771ea-5a46-4c7e-c347-13ede719128c"
      },
      "source": [
        "#randomly shuffle data and do a train-test split on values and labels using the new features\n",
        "features, labels = shuffle(features, labels,random_state=1) \n",
        "X_train = features[:700, :]\n",
        "X_test = features[700:, :]\n",
        "y_train = labels[:700]\n",
        "y_test = labels[700:]\n",
        "\n",
        "#build the fully-connected model\n",
        "fullyConnected = keras.models.Sequential([\n",
        "  layers.Dense(256, activation='relu', input_shape=(128,)),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(16, activation='relu'),\n",
        "  layers.Dense(2, activation='softmax'),\n",
        "])\n",
        "\n",
        "#compile the model\n",
        "fullyConnected.compile(\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#train the model\n",
        "fullyConnected.fit(X_train, keras.utils.to_categorical(y_train), epochs=200,batch_size=8,)\n",
        "\n",
        "#test the model and make final predictions\n",
        "fullyConnected.evaluate(\n",
        "  X_test,\n",
        "  keras.utils.to_categorical(y_test)\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 700 samples\n",
            "Epoch 1/200\n",
            "700/700 [==============================] - 0s 360us/sample - loss: 0.7418 - acc: 0.6029\n",
            "Epoch 2/200\n",
            "700/700 [==============================] - 0s 415us/sample - loss: 0.5270 - acc: 0.7357\n",
            "Epoch 3/200\n",
            "700/700 [==============================] - 0s 378us/sample - loss: 0.3631 - acc: 0.8471\n",
            "Epoch 4/200\n",
            "700/700 [==============================] - 0s 385us/sample - loss: 0.2696 - acc: 0.9057\n",
            "Epoch 5/200\n",
            "700/700 [==============================] - 0s 381us/sample - loss: 0.2007 - acc: 0.9486\n",
            "Epoch 6/200\n",
            "700/700 [==============================] - 0s 387us/sample - loss: 0.1558 - acc: 0.9671\n",
            "Epoch 7/200\n",
            "700/700 [==============================] - 0s 358us/sample - loss: 0.1264 - acc: 0.9814\n",
            "Epoch 8/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 0.1020 - acc: 0.9871\n",
            "Epoch 9/200\n",
            "700/700 [==============================] - 0s 379us/sample - loss: 0.0862 - acc: 0.9871\n",
            "Epoch 10/200\n",
            "700/700 [==============================] - 0s 405us/sample - loss: 0.0726 - acc: 0.9900\n",
            "Epoch 11/200\n",
            "700/700 [==============================] - 0s 364us/sample - loss: 0.0626 - acc: 0.9957\n",
            "Epoch 12/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 0.0531 - acc: 0.9957\n",
            "Epoch 13/200\n",
            "700/700 [==============================] - 0s 397us/sample - loss: 0.0461 - acc: 0.9971\n",
            "Epoch 14/200\n",
            "700/700 [==============================] - 0s 372us/sample - loss: 0.0429 - acc: 0.9957\n",
            "Epoch 15/200\n",
            "700/700 [==============================] - 0s 372us/sample - loss: 0.0395 - acc: 0.9971\n",
            "Epoch 16/200\n",
            "700/700 [==============================] - 0s 355us/sample - loss: 0.0311 - acc: 0.9986\n",
            "Epoch 17/200\n",
            "700/700 [==============================] - 0s 401us/sample - loss: 0.0313 - acc: 1.0000\n",
            "Epoch 18/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 0.0258 - acc: 1.0000\n",
            "Epoch 19/200\n",
            "700/700 [==============================] - 0s 362us/sample - loss: 0.0238 - acc: 0.9986\n",
            "Epoch 20/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 0.0203 - acc: 0.9986\n",
            "Epoch 21/200\n",
            "700/700 [==============================] - 0s 409us/sample - loss: 0.0195 - acc: 0.9986\n",
            "Epoch 22/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 0.0168 - acc: 1.0000\n",
            "Epoch 23/200\n",
            "700/700 [==============================] - 0s 362us/sample - loss: 0.0163 - acc: 0.9986\n",
            "Epoch 24/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 0.0138 - acc: 1.0000\n",
            "Epoch 25/200\n",
            "700/700 [==============================] - 0s 402us/sample - loss: 0.0138 - acc: 0.9986\n",
            "Epoch 26/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 0.0133 - acc: 1.0000\n",
            "Epoch 27/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 0.0110 - acc: 1.0000\n",
            "Epoch 28/200\n",
            "700/700 [==============================] - 0s 386us/sample - loss: 0.0098 - acc: 1.0000\n",
            "Epoch 29/200\n",
            "700/700 [==============================] - 0s 375us/sample - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 30/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 0.0082 - acc: 1.0000\n",
            "Epoch 31/200\n",
            "700/700 [==============================] - 0s 358us/sample - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 32/200\n",
            "700/700 [==============================] - 0s 396us/sample - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 33/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 34/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 35/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 36/200\n",
            "700/700 [==============================] - 0s 400us/sample - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 37/200\n",
            "700/700 [==============================] - 0s 356us/sample - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 38/200\n",
            "700/700 [==============================] - 0s 401us/sample - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 39/200\n",
            "700/700 [==============================] - 0s 370us/sample - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 40/200\n",
            "700/700 [==============================] - 0s 394us/sample - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 41/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 42/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 43/200\n",
            "700/700 [==============================] - 0s 381us/sample - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 44/200\n",
            "700/700 [==============================] - 0s 384us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 45/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 46/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 47/200\n",
            "700/700 [==============================] - 0s 370us/sample - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 48/200\n",
            "700/700 [==============================] - 0s 379us/sample - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 49/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 50/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 51/200\n",
            "700/700 [==============================] - 0s 396us/sample - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 52/200\n",
            "700/700 [==============================] - 0s 347us/sample - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 53/200\n",
            "700/700 [==============================] - 0s 364us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 54/200\n",
            "700/700 [==============================] - 0s 369us/sample - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 55/200\n",
            "700/700 [==============================] - 0s 415us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 56/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 57/200\n",
            "700/700 [==============================] - 0s 377us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 58/200\n",
            "700/700 [==============================] - 0s 386us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 59/200\n",
            "700/700 [==============================] - 0s 401us/sample - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 60/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 61/200\n",
            "700/700 [==============================] - 0s 388us/sample - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 62/200\n",
            "700/700 [==============================] - 0s 375us/sample - loss: 9.2055e-04 - acc: 1.0000\n",
            "Epoch 63/200\n",
            "700/700 [==============================] - 0s 378us/sample - loss: 8.8860e-04 - acc: 1.0000\n",
            "Epoch 64/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 7.8211e-04 - acc: 1.0000\n",
            "Epoch 65/200\n",
            "700/700 [==============================] - 0s 367us/sample - loss: 7.4769e-04 - acc: 1.0000\n",
            "Epoch 66/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 7.3420e-04 - acc: 1.0000\n",
            "Epoch 67/200\n",
            "700/700 [==============================] - 0s 390us/sample - loss: 8.1129e-04 - acc: 1.0000\n",
            "Epoch 68/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 6.1614e-04 - acc: 1.0000\n",
            "Epoch 69/200\n",
            "700/700 [==============================] - 0s 367us/sample - loss: 5.7436e-04 - acc: 1.0000\n",
            "Epoch 70/200\n",
            "700/700 [==============================] - 0s 402us/sample - loss: 5.5747e-04 - acc: 1.0000\n",
            "Epoch 71/200\n",
            "700/700 [==============================] - 0s 361us/sample - loss: 4.9673e-04 - acc: 1.0000\n",
            "Epoch 72/200\n",
            "700/700 [==============================] - 0s 360us/sample - loss: 4.9202e-04 - acc: 1.0000\n",
            "Epoch 73/200\n",
            "700/700 [==============================] - 0s 351us/sample - loss: 4.5979e-04 - acc: 1.0000\n",
            "Epoch 74/200\n",
            "700/700 [==============================] - 0s 383us/sample - loss: 4.3823e-04 - acc: 1.0000\n",
            "Epoch 75/200\n",
            "700/700 [==============================] - 0s 356us/sample - loss: 4.2537e-04 - acc: 1.0000\n",
            "Epoch 76/200\n",
            "700/700 [==============================] - 0s 391us/sample - loss: 3.9470e-04 - acc: 1.0000\n",
            "Epoch 77/200\n",
            "700/700 [==============================] - 0s 406us/sample - loss: 5.6540e-04 - acc: 1.0000\n",
            "Epoch 78/200\n",
            "700/700 [==============================] - 0s 385us/sample - loss: 3.5746e-04 - acc: 1.0000\n",
            "Epoch 79/200\n",
            "700/700 [==============================] - 0s 390us/sample - loss: 3.3442e-04 - acc: 1.0000\n",
            "Epoch 80/200\n",
            "700/700 [==============================] - 0s 381us/sample - loss: 3.3360e-04 - acc: 1.0000\n",
            "Epoch 81/200\n",
            "700/700 [==============================] - 0s 372us/sample - loss: 2.9265e-04 - acc: 1.0000\n",
            "Epoch 82/200\n",
            "700/700 [==============================] - 0s 389us/sample - loss: 2.9491e-04 - acc: 1.0000\n",
            "Epoch 83/200\n",
            "700/700 [==============================] - 0s 374us/sample - loss: 2.7087e-04 - acc: 1.0000\n",
            "Epoch 84/200\n",
            "700/700 [==============================] - 0s 383us/sample - loss: 2.5491e-04 - acc: 1.0000\n",
            "Epoch 85/200\n",
            "700/700 [==============================] - 0s 387us/sample - loss: 2.0981e-04 - acc: 1.0000\n",
            "Epoch 86/200\n",
            "700/700 [==============================] - 0s 361us/sample - loss: 2.3084e-04 - acc: 1.0000\n",
            "Epoch 87/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 1.8266e-04 - acc: 1.0000\n",
            "Epoch 88/200\n",
            "700/700 [==============================] - 0s 361us/sample - loss: 1.9258e-04 - acc: 1.0000\n",
            "Epoch 89/200\n",
            "700/700 [==============================] - 0s 367us/sample - loss: 1.6718e-04 - acc: 1.0000\n",
            "Epoch 90/200\n",
            "700/700 [==============================] - 0s 347us/sample - loss: 1.8059e-04 - acc: 1.0000\n",
            "Epoch 91/200\n",
            "700/700 [==============================] - 0s 383us/sample - loss: 1.6528e-04 - acc: 1.0000\n",
            "Epoch 92/200\n",
            "700/700 [==============================] - 0s 374us/sample - loss: 1.4938e-04 - acc: 1.0000\n",
            "Epoch 93/200\n",
            "700/700 [==============================] - 0s 381us/sample - loss: 2.4355e-04 - acc: 1.0000\n",
            "Epoch 94/200\n",
            "700/700 [==============================] - 0s 362us/sample - loss: 1.4758e-04 - acc: 1.0000\n",
            "Epoch 95/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 1.6892e-04 - acc: 1.0000\n",
            "Epoch 96/200\n",
            "700/700 [==============================] - 0s 360us/sample - loss: 1.1302e-04 - acc: 1.0000\n",
            "Epoch 97/200\n",
            "700/700 [==============================] - 0s 377us/sample - loss: 1.1674e-04 - acc: 1.0000\n",
            "Epoch 98/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 9.9611e-05 - acc: 1.0000\n",
            "Epoch 99/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 1.0059e-04 - acc: 1.0000\n",
            "Epoch 100/200\n",
            "700/700 [==============================] - 0s 367us/sample - loss: 8.6798e-05 - acc: 1.0000\n",
            "Epoch 101/200\n",
            "700/700 [==============================] - 0s 386us/sample - loss: 9.4272e-05 - acc: 1.0000\n",
            "Epoch 102/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 9.3516e-05 - acc: 1.0000\n",
            "Epoch 103/200\n",
            "700/700 [==============================] - 0s 367us/sample - loss: 7.7620e-05 - acc: 1.0000\n",
            "Epoch 104/200\n",
            "700/700 [==============================] - 0s 361us/sample - loss: 8.4576e-05 - acc: 1.0000\n",
            "Epoch 105/200\n",
            "700/700 [==============================] - 0s 387us/sample - loss: 7.0849e-05 - acc: 1.0000\n",
            "Epoch 106/200\n",
            "700/700 [==============================] - 0s 358us/sample - loss: 6.5606e-05 - acc: 1.0000\n",
            "Epoch 107/200\n",
            "700/700 [==============================] - 0s 358us/sample - loss: 6.6871e-05 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "700/700 [==============================] - 0s 365us/sample - loss: 6.4938e-05 - acc: 1.0000\n",
            "Epoch 109/200\n",
            "700/700 [==============================] - 0s 391us/sample - loss: 5.5941e-05 - acc: 1.0000\n",
            "Epoch 110/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 5.0440e-05 - acc: 1.0000\n",
            "Epoch 111/200\n",
            "700/700 [==============================] - 0s 365us/sample - loss: 5.3677e-05 - acc: 1.0000\n",
            "Epoch 112/200\n",
            "700/700 [==============================] - 0s 359us/sample - loss: 5.7316e-05 - acc: 1.0000\n",
            "Epoch 113/200\n",
            "700/700 [==============================] - 0s 389us/sample - loss: 5.0343e-05 - acc: 1.0000\n",
            "Epoch 114/200\n",
            "700/700 [==============================] - 0s 360us/sample - loss: 3.9086e-05 - acc: 1.0000\n",
            "Epoch 115/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 4.1582e-05 - acc: 1.0000\n",
            "Epoch 116/200\n",
            "700/700 [==============================] - 0s 427us/sample - loss: 4.2799e-05 - acc: 1.0000\n",
            "Epoch 117/200\n",
            "700/700 [==============================] - 0s 396us/sample - loss: 4.0512e-05 - acc: 1.0000\n",
            "Epoch 118/200\n",
            "700/700 [==============================] - 0s 389us/sample - loss: 3.5943e-05 - acc: 1.0000\n",
            "Epoch 119/200\n",
            "700/700 [==============================] - 0s 435us/sample - loss: 5.2885e-05 - acc: 1.0000\n",
            "Epoch 120/200\n",
            "700/700 [==============================] - 0s 412us/sample - loss: 3.9554e-05 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "700/700 [==============================] - 0s 410us/sample - loss: 2.6728e-05 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "700/700 [==============================] - 0s 411us/sample - loss: 2.7943e-05 - acc: 1.0000\n",
            "Epoch 123/200\n",
            "700/700 [==============================] - 0s 422us/sample - loss: 2.6249e-05 - acc: 1.0000\n",
            "Epoch 124/200\n",
            "700/700 [==============================] - 0s 389us/sample - loss: 2.5479e-05 - acc: 1.0000\n",
            "Epoch 125/200\n",
            "700/700 [==============================] - 0s 411us/sample - loss: 2.7840e-05 - acc: 1.0000\n",
            "Epoch 126/200\n",
            "700/700 [==============================] - 0s 398us/sample - loss: 2.1934e-05 - acc: 1.0000\n",
            "Epoch 127/200\n",
            "700/700 [==============================] - 0s 442us/sample - loss: 1.9830e-05 - acc: 1.0000\n",
            "Epoch 128/200\n",
            "700/700 [==============================] - 0s 396us/sample - loss: 1.9220e-05 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "700/700 [==============================] - 0s 403us/sample - loss: 1.9378e-05 - acc: 1.0000\n",
            "Epoch 130/200\n",
            "700/700 [==============================] - 0s 389us/sample - loss: 1.7626e-05 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "700/700 [==============================] - 0s 395us/sample - loss: 1.7939e-05 - acc: 1.0000\n",
            "Epoch 132/200\n",
            "700/700 [==============================] - 0s 367us/sample - loss: 2.3147e-05 - acc: 1.0000\n",
            "Epoch 133/200\n",
            "700/700 [==============================] - 0s 380us/sample - loss: 1.3488e-05 - acc: 1.0000\n",
            "Epoch 134/200\n",
            "700/700 [==============================] - 0s 399us/sample - loss: 2.1414e-05 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "700/700 [==============================] - 0s 362us/sample - loss: 1.4680e-05 - acc: 1.0000\n",
            "Epoch 136/200\n",
            "700/700 [==============================] - 0s 395us/sample - loss: 1.3396e-05 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "700/700 [==============================] - 0s 374us/sample - loss: 1.1625e-05 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "700/700 [==============================] - 0s 379us/sample - loss: 1.1105e-05 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "700/700 [==============================] - 0s 362us/sample - loss: 1.0677e-05 - acc: 1.0000\n",
            "Epoch 140/200\n",
            "700/700 [==============================] - 0s 374us/sample - loss: 9.6046e-06 - acc: 1.0000\n",
            "Epoch 141/200\n",
            "700/700 [==============================] - 0s 426us/sample - loss: 1.1115e-05 - acc: 1.0000\n",
            "Epoch 142/200\n",
            "700/700 [==============================] - 0s 364us/sample - loss: 9.4726e-06 - acc: 1.0000\n",
            "Epoch 143/200\n",
            "700/700 [==============================] - 0s 360us/sample - loss: 8.7134e-06 - acc: 1.0000\n",
            "Epoch 144/200\n",
            "700/700 [==============================] - 0s 378us/sample - loss: 8.4384e-06 - acc: 1.0000\n",
            "Epoch 145/200\n",
            "700/700 [==============================] - 0s 392us/sample - loss: 9.3672e-06 - acc: 1.0000\n",
            "Epoch 146/200\n",
            "700/700 [==============================] - 0s 359us/sample - loss: 8.2293e-06 - acc: 1.0000\n",
            "Epoch 147/200\n",
            "700/700 [==============================] - 0s 373us/sample - loss: 8.8331e-06 - acc: 1.0000\n",
            "Epoch 148/200\n",
            "700/700 [==============================] - 0s 386us/sample - loss: 7.2292e-06 - acc: 1.0000\n",
            "Epoch 149/200\n",
            "700/700 [==============================] - 0s 391us/sample - loss: 6.7783e-06 - acc: 1.0000\n",
            "Epoch 150/200\n",
            "700/700 [==============================] - 0s 357us/sample - loss: 5.7733e-06 - acc: 1.0000\n",
            "Epoch 151/200\n",
            "700/700 [==============================] - 0s 388us/sample - loss: 5.6381e-06 - acc: 1.0000\n",
            "Epoch 152/200\n",
            "700/700 [==============================] - 0s 365us/sample - loss: 5.7507e-06 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "700/700 [==============================] - 0s 388us/sample - loss: 5.5752e-06 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "700/700 [==============================] - 0s 356us/sample - loss: 5.3585e-06 - acc: 1.0000\n",
            "Epoch 155/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 4.6724e-06 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "700/700 [==============================] - 0s 385us/sample - loss: 4.5730e-06 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "700/700 [==============================] - 0s 377us/sample - loss: 4.6979e-06 - acc: 1.0000\n",
            "Epoch 158/200\n",
            "700/700 [==============================] - 0s 370us/sample - loss: 4.1667e-06 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "700/700 [==============================] - 0s 382us/sample - loss: 3.7236e-06 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "700/700 [==============================] - 0s 394us/sample - loss: 3.7256e-06 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "700/700 [==============================] - 0s 405us/sample - loss: 3.7070e-06 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "700/700 [==============================] - 0s 392us/sample - loss: 3.6999e-06 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "700/700 [==============================] - 0s 369us/sample - loss: 3.4414e-06 - acc: 1.0000\n",
            "Epoch 164/200\n",
            "700/700 [==============================] - 0s 389us/sample - loss: 2.9628e-06 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "700/700 [==============================] - 0s 379us/sample - loss: 2.9319e-06 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "700/700 [==============================] - 0s 373us/sample - loss: 2.9381e-06 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "700/700 [==============================] - 0s 366us/sample - loss: 2.4478e-06 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "700/700 [==============================] - 0s 369us/sample - loss: 3.0046e-06 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "700/700 [==============================] - 0s 353us/sample - loss: 2.2908e-06 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "700/700 [==============================] - 0s 387us/sample - loss: 2.0878e-06 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "700/700 [==============================] - 0s 385us/sample - loss: 2.2479e-06 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "700/700 [==============================] - 0s 384us/sample - loss: 2.0355e-06 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "700/700 [==============================] - 0s 378us/sample - loss: 1.7098e-06 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "700/700 [==============================] - 0s 386us/sample - loss: 1.9419e-06 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "700/700 [==============================] - 0s 391us/sample - loss: 2.3259e-06 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "700/700 [==============================] - 0s 365us/sample - loss: 1.6297e-06 - acc: 1.0000\n",
            "Epoch 177/200\n",
            "700/700 [==============================] - 0s 382us/sample - loss: 1.5957e-06 - acc: 1.0000\n",
            "Epoch 178/200\n",
            "700/700 [==============================] - 0s 368us/sample - loss: 1.4979e-06 - acc: 1.0000\n",
            "Epoch 179/200\n",
            "700/700 [==============================] - 0s 411us/sample - loss: 1.4887e-06 - acc: 1.0000\n",
            "Epoch 180/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 1.3419e-06 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "700/700 [==============================] - 0s 385us/sample - loss: 1.1934e-06 - acc: 1.0000\n",
            "Epoch 182/200\n",
            "700/700 [==============================] - 0s 364us/sample - loss: 1.1887e-06 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "700/700 [==============================] - 0s 386us/sample - loss: 1.0504e-06 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "700/700 [==============================] - 0s 384us/sample - loss: 1.0506e-06 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "700/700 [==============================] - 0s 377us/sample - loss: 9.9113e-07 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "700/700 [==============================] - 0s 393us/sample - loss: 9.5213e-07 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "700/700 [==============================] - 0s 376us/sample - loss: 8.5302e-07 - acc: 1.0000\n",
            "Epoch 188/200\n",
            "700/700 [==============================] - 0s 355us/sample - loss: 8.6153e-07 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "700/700 [==============================] - 0s 396us/sample - loss: 8.2237e-07 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "700/700 [==============================] - 0s 394us/sample - loss: 7.9631e-07 - acc: 1.0000\n",
            "Epoch 191/200\n",
            "700/700 [==============================] - 0s 362us/sample - loss: 6.9618e-07 - acc: 1.0000\n",
            "Epoch 192/200\n",
            "700/700 [==============================] - 0s 375us/sample - loss: 6.7574e-07 - acc: 1.0000\n",
            "Epoch 193/200\n",
            "700/700 [==============================] - 0s 397us/sample - loss: 6.6518e-07 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "700/700 [==============================] - 0s 431us/sample - loss: 6.0916e-07 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "700/700 [==============================] - 0s 363us/sample - loss: 6.4747e-07 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "700/700 [==============================] - 0s 396us/sample - loss: 5.4870e-07 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "700/700 [==============================] - 0s 369us/sample - loss: 5.6249e-07 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "700/700 [==============================] - 0s 382us/sample - loss: 4.7394e-07 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "700/700 [==============================] - 0s 360us/sample - loss: 4.6968e-07 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "700/700 [==============================] - 0s 393us/sample - loss: 5.3303e-07 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5273665877052426, 0.94265234]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}