{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSE106291.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAIxcB-lf7y2",
        "colab_type": "code",
        "outputId": "9a493bba-2355-421a-ea24-03ecc85bdaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#import necessary packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJWe4oQWDbQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset tf graph\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEs1wF5WiX_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read data and split labels from values\n",
        "initial = (pd.read_csv('/content/drive/My Drive/GSE106291_Matrix_table_preprocessed.csv')).T\n",
        "initial=initial.iloc[1:]\n",
        "values = initial.values\n",
        "labels = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,0,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,0,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,1,0,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,1,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,1,0,1,1,0,1,1,0,1,1,0,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxEhsx5mB2_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#randomly shuffle data and do a train-test split on values and labels\n",
        "values, labels = shuffle(values, labels,random_state=1) \n",
        "values_train = values[:200, :]\n",
        "values_test = values[200:, :]\n",
        "y_train = labels[:200]\n",
        "y_test = labels[200:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlYw0jYPizOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the RF with 500 trees and make predictions\n",
        "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rf.fit(values_train, y_train)\n",
        "acc_rf = 1 - sum(abs(rf.predict(values_test)-y_test))/len(y_test)\n",
        "x_total = [tree.predict(values) for tree in rf.estimators_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhY6QruMi_J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot encode the features \n",
        "x_total = np.transpose(x_total)\n",
        "x_train = []\n",
        "for sample in x_total:\n",
        "    s = []\n",
        "    for feature in sample:\n",
        "        if feature == 1:\n",
        "            s.append([0, 1])\n",
        "        else:\n",
        "            s.append([1, 0])\n",
        "    x_train.append(s)\n",
        "x_train = np.array(x_train)\n",
        "x_test = x_train[200:, :] \n",
        "x_train = x_train[:200, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MQg3o2RoLJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot encode the labels\n",
        "labelss = []\n",
        "for l in labels:\n",
        "    if l == 1:\n",
        "        labelss.append([0, 1])\n",
        "    else:\n",
        "        labelss.append([1, 0])\n",
        "labelss = np.array(labelss,dtype=int)\n",
        "\n",
        "y_train = labelss[:200, :]\n",
        "y_test = labelss[200:, :]\n",
        "auc_rf = metrics.roc_auc_score(y_test, rf.predict_proba(values_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht6cZpW_obEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nn parameter initialization\n",
        "L2 = True \n",
        "droph1 = False\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 200\n",
        "batch_size = 8\n",
        "display_step = 1\n",
        "\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2 = 64\n",
        "n_hidden_3 = 16\n",
        "n_classes = 2\n",
        "n_features = np.shape(x_train)[1]\n",
        "\n",
        "loss_rec = np.zeros([training_epochs, 1])\n",
        "training_eval = np.zeros([training_epochs, 2])\n",
        "\n",
        "avg_test_acc = 0.\n",
        "avg_test_auc = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex5R2029olIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the nn architecture\n",
        "def multilayer_perceptron(x, weights, biases, keep_prob):\n",
        "\n",
        "    layer_1 = tf.add(tf.tensordot(x, weights['h1'], axes=[[1, 2],[0, 1]]), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    if droph1:\n",
        "        layer_1 = tf.nn.dropout(layer_1, keep_prob=keep_prob)\n",
        "\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_2 = tf.nn.dropout(layer_2, keep_prob=keep_prob)\n",
        "\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    layer_3 = tf.nn.dropout(layer_3, keep_prob=keep_prob)\n",
        "\n",
        "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDVd_0vDoq2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weights and biases\n",
        "x = tf.placeholder(tf.float32, [None, n_features, 2])\n",
        "y = tf.placeholder(tf.int32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "lr = tf.placeholder(tf.float32)\n",
        "\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.truncated_normal(shape=[2, n_features, n_hidden_1], stddev=0.1)),\n",
        "    'h2': tf.Variable(tf.truncated_normal(shape=[n_hidden_1, n_hidden_2], stddev=0.1)),\n",
        "    'h3': tf.Variable(tf.truncated_normal(shape=[n_hidden_2, n_hidden_3], stddev=0.1)),\n",
        "    'out': tf.Variable(tf.truncated_normal(shape=[n_hidden_3, n_classes], stddev=0.1))\n",
        "\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.zeros([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.zeros([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIzWPg_no2z_",
        "colab_type": "code",
        "outputId": "0d5ea4fe-5398-45d7-b78f-77da63d6430a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#build the model\n",
        "pred = multilayer_perceptron(x, weights, biases, keep_prob)\n",
        "\n",
        "#model cost and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "if L2:\n",
        "    reg = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2']) + \\\n",
        "          tf.nn.l2_loss(weights['h3']) + tf.nn.l2_loss(weights['out'])\n",
        "    cost = tf.reduce_mean(cost + 0.1 * reg)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
        "\n",
        "#evaluation\n",
        "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "y_score = tf.nn.softmax(logits=pred)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-376c6eaf6edf>:10: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-11-f7ad20c404fd>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhtQ5bazr01L",
        "colab_type": "code",
        "outputId": "29677271-feb5-4498-9e59-db33ee6a7716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train the model\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    total_batch = int(np.shape(x_train)[0] / batch_size)\n",
        "\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        x_tmp, y_tmp = shuffle(x_train, y_train,random_state=1)\n",
        "        \n",
        "        for i in range(total_batch-1):\n",
        "            batch_x, batch_y = x_tmp[i*batch_size:i*batch_size+batch_size], \\\n",
        "                                y_tmp[i*batch_size:i*batch_size+batch_size]\n",
        "            _, c= sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y,\n",
        "                                                        keep_prob: 0.9,\n",
        "                                                        lr: learning_rate\n",
        "                                                        })  \n",
        "            avg_cost += c / total_batch\n",
        "\n",
        "        del x_tmp\n",
        "        del y_tmp\n",
        "\n",
        "        if epoch % display_step == 0:\n",
        "            loss_rec[epoch] = avg_cost\n",
        "            acc, y_s = sess.run([accuracy, y_score], feed_dict={x: x_train, y: y_train, keep_prob: 1})\n",
        "            auc = metrics.roc_auc_score(y_train, y_s)\n",
        "            training_eval[epoch] = [acc, auc]\n",
        "            print (\"Epoch:\", '%d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost),\n",
        "                    \"Training accuracy:\", round(acc,3), \" Training auc:\", round(auc,3))\n",
        "        if avg_cost <= 0.1:\n",
        "            break  \n",
        "\n",
        "#test the model and evaluate\n",
        "    acc, y_s = sess.run([accuracy, y_score], feed_dict={x: x_test, y: y_test, keep_prob: 1})\n",
        "    auc = metrics.roc_auc_score(y_test, y_s)\n",
        "    print(\"*****=====\", \"Testing accuracy: \", acc, \" Testing auc: \", auc, \"=====*****\")\n",
        "    print(auc)\n",
        "    print(auc_rf)                                                   "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 cost = 100.092902832 Training accuracy: 0.945  Training auc: 1.0\n",
            "Epoch: 2 cost = 95.692811279 Training accuracy: 0.985  Training auc: 1.0\n",
            "Epoch: 3 cost = 91.516100464 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 4 cost = 87.534865723 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 5 cost = 83.724122009 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 6 cost = 80.095392761 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 7 cost = 76.615394287 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 8 cost = 73.276277771 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 9 cost = 70.097014771 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 10 cost = 67.045455933 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 11 cost = 64.139557495 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 12 cost = 61.339464722 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 13 cost = 58.663179932 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 14 cost = 56.104845428 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 15 cost = 53.651265564 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 16 cost = 51.307598114 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 17 cost = 49.047106018 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 18 cost = 46.899720154 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 19 cost = 44.825839233 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 20 cost = 42.848933716 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 21 cost = 40.958081665 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 22 cost = 39.141471252 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 23 cost = 37.407849884 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 24 cost = 35.730895538 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 25 cost = 34.140015411 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 26 cost = 32.611452332 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 27 cost = 31.147420349 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 28 cost = 29.744672623 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 29 cost = 28.404927902 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 30 cost = 27.111673431 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 31 cost = 25.882865524 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 32 cost = 24.698199005 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 33 cost = 23.564007568 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 34 cost = 22.477556229 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 35 cost = 21.445304565 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 36 cost = 20.449165344 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 37 cost = 19.501326370 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 38 cost = 18.591561966 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 39 cost = 17.721953964 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 40 cost = 16.888943787 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 41 cost = 16.087819138 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 42 cost = 15.331847458 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 43 cost = 14.599659462 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 44 cost = 13.899924660 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 45 cost = 13.229013481 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 46 cost = 12.593757591 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 47 cost = 11.984584694 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 48 cost = 11.398354645 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 49 cost = 10.845340271 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 50 cost = 10.310975418 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 51 cost = 9.804864807 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 52 cost = 9.320180130 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 53 cost = 8.855466270 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 54 cost = 8.414762726 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 55 cost = 7.991930275 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 56 cost = 7.583992977 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 57 cost = 7.204864826 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 58 cost = 6.839943962 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 59 cost = 6.488035278 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 60 cost = 6.160659084 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 61 cost = 5.838285389 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 62 cost = 5.537469788 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 63 cost = 5.250127258 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 64 cost = 4.976148663 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 65 cost = 4.713101940 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 66 cost = 4.462526264 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 67 cost = 4.226979790 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 68 cost = 4.002136555 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 69 cost = 3.787447405 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 70 cost = 3.583476610 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 71 cost = 3.392311726 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 72 cost = 3.207401276 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 73 cost = 3.033543072 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 74 cost = 2.869430046 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 75 cost = 2.712854319 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 76 cost = 2.566463795 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 77 cost = 2.424299803 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 78 cost = 2.287907505 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 79 cost = 2.162926865 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 80 cost = 2.040886841 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 81 cost = 1.927852983 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 82 cost = 1.818303089 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 83 cost = 1.717784166 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 84 cost = 1.621314750 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 85 cost = 1.529612494 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 86 cost = 1.447068195 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 87 cost = 1.366268525 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 88 cost = 1.287391796 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 89 cost = 1.216806808 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 90 cost = 1.147621679 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 91 cost = 1.082163768 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 92 cost = 1.021242099 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 93 cost = 0.966558778 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 94 cost = 0.911716766 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 95 cost = 0.861285498 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 96 cost = 0.815294628 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 97 cost = 0.769054294 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 98 cost = 0.729953842 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 99 cost = 0.689887674 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 100 cost = 0.653654211 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 101 cost = 0.616606383 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 102 cost = 0.581747432 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 103 cost = 0.554203777 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 104 cost = 0.526905282 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 105 cost = 0.498111455 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 106 cost = 0.472456393 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 107 cost = 0.450490841 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 108 cost = 0.431422865 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 109 cost = 0.410415668 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 110 cost = 0.390027813 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 111 cost = 0.371795704 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 112 cost = 0.356517782 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 113 cost = 0.339224396 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 114 cost = 0.325700054 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 115 cost = 0.311097119 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 116 cost = 0.299903946 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 117 cost = 0.288255121 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 118 cost = 0.275732898 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 119 cost = 0.269665351 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 120 cost = 0.261491328 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 121 cost = 0.250886459 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 122 cost = 0.243099425 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 123 cost = 0.237004001 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 124 cost = 0.228434526 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 125 cost = 0.223197316 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 126 cost = 0.215782974 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 127 cost = 0.211959219 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 128 cost = 0.206079502 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 129 cost = 0.202929080 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 130 cost = 0.199225984 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 131 cost = 0.194712900 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 132 cost = 0.189121525 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 133 cost = 0.188055332 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 134 cost = 0.186232200 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 135 cost = 0.181470733 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 136 cost = 0.180994548 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 137 cost = 0.176249059 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 138 cost = 0.174251516 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 139 cost = 0.170552111 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 140 cost = 0.173069496 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 141 cost = 0.168753558 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 142 cost = 0.168761691 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 143 cost = 0.168458455 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 144 cost = 0.165006918 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 145 cost = 0.164648364 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 146 cost = 0.164950910 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 147 cost = 0.163403861 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 148 cost = 0.162188961 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 149 cost = 0.159360934 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 150 cost = 0.160031765 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 151 cost = 0.160247849 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 152 cost = 0.158289978 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 153 cost = 0.157823416 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 154 cost = 0.156309138 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 155 cost = 0.157121793 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 156 cost = 0.157322989 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 157 cost = 0.155605691 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 158 cost = 0.158040259 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 159 cost = 0.156150367 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 160 cost = 0.154423521 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 161 cost = 0.153779043 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 162 cost = 0.153344283 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 163 cost = 0.153458484 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 164 cost = 0.152218745 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 165 cost = 0.154794053 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 166 cost = 0.152321492 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 167 cost = 0.153364696 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 168 cost = 0.153231396 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 169 cost = 0.155058408 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 170 cost = 0.152491239 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 171 cost = 0.152079414 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 172 cost = 0.152821029 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 173 cost = 0.152058304 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 174 cost = 0.153239090 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 175 cost = 0.150446833 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 176 cost = 0.150946661 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 177 cost = 0.151730826 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 178 cost = 0.151270253 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 179 cost = 0.152668633 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 180 cost = 0.150719486 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 181 cost = 0.149652565 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 182 cost = 0.151102995 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 183 cost = 0.150533947 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 184 cost = 0.154391218 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 185 cost = 0.149205654 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 186 cost = 0.151082863 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 187 cost = 0.150161387 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 188 cost = 0.149694054 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 189 cost = 0.150669807 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 190 cost = 0.149639404 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 191 cost = 0.148671310 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 192 cost = 0.148676404 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 193 cost = 0.149138733 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 194 cost = 0.149708334 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 195 cost = 0.152340226 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 196 cost = 0.147579864 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 197 cost = 0.148935688 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 198 cost = 0.150418153 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 199 cost = 0.148945047 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 200 cost = 0.148953158 Training accuracy: 1.0  Training auc: 1.0\n",
            "*****===== Testing accuracy:  0.8  Testing auc:  0.7422037422037422 =====*****\n",
            "0.7422037422037422\n",
            "0.7494802494802495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-MrToLEE7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "values_train = values[:200, :]\n",
        "values_test = values[200:, :]\n",
        "y_train = labels[:200]\n",
        "y_test = labels[200:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycKT0N-nEFUi",
        "colab_type": "code",
        "outputId": "ade31f44-53e8-4d82-fb95-9408dc4c0d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#RF with 1000 trees\n",
        "rf2 = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
        "rf2.fit(values_train, y_train)\n",
        "acc_rf2 = 1 - sum(abs(rf2.predict(values_test)-y_test))/len(y_test)\n",
        "auc_rf2 = metrics.roc_auc_score(y_test, rf2.predict(values_test))\n",
        "auc_rf2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6652806652806653"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OghCUlv2EFko",
        "colab_type": "code",
        "outputId": "ffab771c-8668-40ac-d990-40af268c864e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#SVM with Sigmoid kernel\n",
        "clf = svm.SVC(kernel='sigmoid')\n",
        "clf.fit(values_train, y_train)\n",
        "acc_clf = 1 - sum(abs(clf.predict(values_test)-y_test))/len(y_test)\n",
        "auc_clf = metrics.roc_auc_score(y_test, clf.predict(values_test))\n",
        "print(auc_clf)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6767151767151767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVcKBzq8EVJ3",
        "colab_type": "code",
        "outputId": "8918cc66-456d-480c-c8a1-f18282455dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#linear SVM with full features\n",
        "linear_clf = svm.LinearSVC()\n",
        "linear_clf.fit(values_train,y_train)\n",
        "acc_linear_clf = 1 - sum(abs(linear_clf.predict(values_test)-y_test))/len(y_test)\n",
        "auc_linear_clf = metrics.roc_auc_score(y_test, linear_clf.predict(values_test))\n",
        "print(auc_linear_clf)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6122661122661123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kck2d51DEYKf",
        "colab_type": "code",
        "outputId": "812e8711-8df7-4d7b-af27-5e6a2e934700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "#detect important features\n",
        "def plot_coefficients(classifier, top_features=250):\n",
        " coef = classifier.coef_.ravel()\n",
        " top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
        " top_negative_coefficients = np.argsort(coef)[:top_features]\n",
        " top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
        "\n",
        " plt.figure(figsize=(15, 5))\n",
        " colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
        " plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
        " plt.show()\n",
        " return(top_coefficients) \n",
        "\n",
        "\n",
        "top_features = plot_coefficients(linear_clf)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVcEjXQEEcfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Selection\n",
        "compressed = initial.iloc[:,top_features]\n",
        "values = compressed.values\n",
        "values, labels = shuffle(values, labels,random_state=1) \n",
        "values_train2 = values[:200, :]\n",
        "values_test2 = values[200:, :]\n",
        "y_train2 = labels[:200]\n",
        "y_test2 = labels[200:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAx0bFjvEpm6",
        "colab_type": "code",
        "outputId": "099e9c71-6eae-4918-d179-02959b00e10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#linear SVM with selected features\n",
        "linear_clf2 = svm.LinearSVC()\n",
        "linear_clf2.fit(values_train2,y_train2)\n",
        "acc_linear_clf2 = 1 - sum(abs(linear_clf2.predict(values_test2)-y_test2))/len(y_test2)\n",
        "auc_linear_clf2 = metrics.roc_auc_score(y_test2, linear_clf2.predict(values_test2))\n",
        "print(auc_linear_clf2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5735294117647058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB70Cv88KL2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "df746b65-031c-4963-c6c1-22a9fefc420f"
      },
      "source": [
        "#comparison\n",
        "RF_auc = pd.DataFrame({'RF with 500 Tress':[auc_rf],'RF with 1000 Tress':[auc_rf2]})\n",
        "fdnn_svd_auc = pd.DataFrame({'fDNN':[auc],'Sigmoid SVM':[auc_clf],'Full Linear SVM':[auc_linear_clf],'FS Linear SVM':[auc_linear_clf2]})\n",
        "print(RF_auc)\n",
        "print(fdnn_svd_auc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   RF with 500 Tress  RF with 1000 Tress\n",
            "0            0.74948            0.665281\n",
            "       fDNN  Sigmoid SVM  Full Linear SVM  FS Linear SVM\n",
            "0  0.742204     0.676715         0.612266       0.573529\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}