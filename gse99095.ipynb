{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSE99095.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAIxcB-lf7y2",
        "colab_type": "code",
        "outputId": "232579c3-95af-4564-c242-56ee57d0ef14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#import necessary packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJWe4oQWDbQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset tf graph\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEs1wF5WiX_6",
        "colab_type": "code",
        "outputId": "f77c30d6-9f69-4844-c893-505a640121ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#read data and split labels from values\n",
        "initial = (pd.read_csv('/content/drive/My Drive/GSE99095_normalizedExpression.csv',header=None)).T\n",
        "#create labels\n",
        "initial[0]=np.zeros(980)\n",
        "for i in range(1,392):\n",
        "    (initial[0].values)[i]=1\n",
        "\n",
        "initial=initial.iloc[1:]\n",
        "labels= initial[0].values\n",
        "values = (initial.drop((initial[0]),axis=1)).values"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxEhsx5mB2_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#randomly shuffle data and do a train-test split on values and labels\n",
        "values, labels = shuffle(values, labels,random_state=1) \n",
        "values_train = values[:700, :]\n",
        "values_test = values[700:, :]\n",
        "y_train = labels[:700]\n",
        "y_test = labels[700:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlYw0jYPizOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the RF with 400 trees and make predictions\n",
        "rf = RandomForestClassifier(n_estimators=400, n_jobs=-1)\n",
        "rf.fit(values_train, y_train)\n",
        "acc_rf = 1 - sum(abs(rf.predict(values_test)-y_test))/len(y_test)\n",
        "\n",
        "x_total = [tree.predict(values) for tree in rf.estimators_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhY6QruMi_J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot encode the features \n",
        "x_total = np.transpose(x_total)\n",
        "x_train = []\n",
        "for sample in x_total:\n",
        "    s = []\n",
        "    for feature in sample:\n",
        "        if feature == 1:\n",
        "            s.append([0, 1])\n",
        "        else:\n",
        "            s.append([1, 0])\n",
        "    x_train.append(s)\n",
        "x_train = np.array(x_train)\n",
        "x_test = x_train[700:, :] \n",
        "x_train = x_train[:700, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MQg3o2RoLJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#one-hot encode the labels\n",
        "labelss = []\n",
        "for l in labels:\n",
        "    if l == 1:\n",
        "        labelss.append([0, 1])\n",
        "    else:\n",
        "        labelss.append([1, 0])\n",
        "labelss = np.array(labelss,dtype=int)\n",
        "\n",
        "y_train = labelss[:700, :]\n",
        "y_test = labelss[700:, :]\n",
        "auc_rf = metrics.roc_auc_score(y_test, rf.predict_proba(values_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht6cZpW_obEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nn parameter initialization\n",
        "L2 = True \n",
        "droph1 = False\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 200\n",
        "batch_size = 8\n",
        "display_step = 1\n",
        "\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2 = 64\n",
        "n_hidden_3 = 16\n",
        "n_classes = 2\n",
        "n_features = np.shape(x_train)[1]\n",
        "\n",
        "loss_rec = np.zeros([training_epochs, 1])\n",
        "training_eval = np.zeros([training_epochs, 2])\n",
        "\n",
        "avg_test_acc = 0.\n",
        "avg_test_auc = 0."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex5R2029olIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the nn architecture\n",
        "def multilayer_perceptron(x, weights, biases, keep_prob):\n",
        "\n",
        "    layer_1 = tf.add(tf.tensordot(x, weights['h1'], axes=[[1, 2],[0, 1]]), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    if droph1:\n",
        "        layer_1 = tf.nn.dropout(layer_1, keep_prob=keep_prob)\n",
        "\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_2 = tf.nn.dropout(layer_2, keep_prob=keep_prob)\n",
        "\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    layer_3 = tf.nn.dropout(layer_3, keep_prob=keep_prob)\n",
        "\n",
        "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDVd_0vDoq2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weights and biases\n",
        "x = tf.placeholder(tf.float32, [None, n_features, 2])\n",
        "y = tf.placeholder(tf.int32, [None, n_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "lr = tf.placeholder(tf.float32)\n",
        "\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.truncated_normal(shape=[2, n_features, n_hidden_1], stddev=0.1)),\n",
        "    'h2': tf.Variable(tf.truncated_normal(shape=[n_hidden_1, n_hidden_2], stddev=0.1)),\n",
        "    'h3': tf.Variable(tf.truncated_normal(shape=[n_hidden_2, n_hidden_3], stddev=0.1)),\n",
        "    'out': tf.Variable(tf.truncated_normal(shape=[n_hidden_3, n_classes], stddev=0.1))\n",
        "\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.zeros([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.zeros([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIzWPg_no2z_",
        "colab_type": "code",
        "outputId": "403854e3-6b92-4a6d-9e42-5f30f0ce375a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "#build the model\n",
        "pred = multilayer_perceptron(x, weights, biases, keep_prob)\n",
        "\n",
        "#model cost and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
        "if L2:\n",
        "    reg = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['h2']) + \\\n",
        "          tf.nn.l2_loss(weights['h3']) + tf.nn.l2_loss(weights['out'])\n",
        "    cost = tf.reduce_mean(cost + 0.1 * reg)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
        "\n",
        "#evaluation\n",
        "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "y_score = tf.nn.softmax(logits=pred)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-376c6eaf6edf>:10: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-11-f7ad20c404fd>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhtQ5bazr01L",
        "colab_type": "code",
        "outputId": "2eb8253b-66aa-4580-c0d2-f1f0716c8fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train the model\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    total_batch = int(np.shape(x_train)[0] / batch_size)\n",
        "\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        x_tmp, y_tmp = shuffle(x_train, y_train,random_state=1)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch-1):\n",
        "            batch_x, batch_y = x_tmp[i*batch_size:i*batch_size+batch_size], \\\n",
        "                                y_tmp[i*batch_size:i*batch_size+batch_size]\n",
        "            _, c= sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y,\n",
        "                                                        keep_prob: 0.9,\n",
        "                                                        lr: learning_rate\n",
        "                                                        })  \n",
        "            avg_cost += c / total_batch\n",
        "\n",
        "        del x_tmp\n",
        "        del y_tmp\n",
        "\n",
        "        if epoch % display_step == 0:\n",
        "            loss_rec[epoch] = avg_cost\n",
        "            acc, y_s = sess.run([accuracy, y_score], feed_dict={x: x_train, y: y_train, keep_prob: 1})\n",
        "            auc = metrics.roc_auc_score(y_train, y_s)\n",
        "            training_eval[epoch] = [acc, auc]\n",
        "            print (\"Epoch:\", '%d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost),\n",
        "                    \"Training accuracy:\", round(acc,3), \" Training auc:\", round(auc,3))\n",
        "        if avg_cost <= 0.1:\n",
        "            break  \n",
        "\n",
        "#test the model and evaluate\n",
        "    acc, y_s = sess.run([accuracy, y_score], feed_dict={x: x_test, y: y_test, keep_prob: 1})\n",
        "    auc = metrics.roc_auc_score(y_test, y_s)\n",
        "    print(\"*****=====\", \"Testing accuracy: \", acc, \" Testing auc: \", auc, \"=====*****\")\n",
        "    print(auc)\n",
        "    print(auc_rf)                                                   "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 cost = 79.342366186 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 2 cost = 67.634769703 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 3 cost = 57.665784112 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 4 cost = 49.150822124 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 5 cost = 41.863398059 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 6 cost = 35.613442081 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 7 cost = 30.260460470 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 8 cost = 25.671587845 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 9 cost = 21.742701563 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 10 cost = 18.368577365 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 11 cost = 15.494101787 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 12 cost = 13.033642725 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 13 cost = 10.937707846 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 14 cost = 9.153740839 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 15 cost = 7.639523484 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 16 cost = 6.359420393 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 17 cost = 5.276208335 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 18 cost = 4.365411150 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 19 cost = 3.603906516 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 20 cost = 2.966267246 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 21 cost = 2.434893016 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 22 cost = 1.993303511 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 23 cost = 1.630145648 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 24 cost = 1.333731862 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 25 cost = 1.090221104 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 26 cost = 0.892683442 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 27 cost = 0.733603373 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 28 cost = 0.604996417 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 29 cost = 0.501886063 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 30 cost = 0.422004782 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 31 cost = 0.357298556 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 32 cost = 0.308287900 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 33 cost = 0.269974072 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 34 cost = 0.240889131 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 35 cost = 0.219959146 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 36 cost = 0.201061656 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 37 cost = 0.189297220 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 38 cost = 0.180856975 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 39 cost = 0.172891690 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 40 cost = 0.168198043 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 41 cost = 0.164733356 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 42 cost = 0.160815788 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 43 cost = 0.159145116 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 44 cost = 0.156964534 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 45 cost = 0.155474517 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 46 cost = 0.154201928 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 47 cost = 0.153410797 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 48 cost = 0.151580980 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 49 cost = 0.150943604 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 50 cost = 0.151428795 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 51 cost = 0.149921414 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 52 cost = 0.151267348 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 53 cost = 0.150101860 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 54 cost = 0.148373885 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 55 cost = 0.148639007 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 56 cost = 0.147477542 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 57 cost = 0.146861807 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 58 cost = 0.146934280 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 59 cost = 0.146330657 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 60 cost = 0.145381761 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 61 cost = 0.145379548 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 62 cost = 0.144386662 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 63 cost = 0.144332823 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 64 cost = 0.143586664 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 65 cost = 0.143572766 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 66 cost = 0.142221336 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 67 cost = 0.141894338 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 68 cost = 0.141972459 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 69 cost = 0.141233647 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 70 cost = 0.139973888 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 71 cost = 0.140039631 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 72 cost = 0.140256733 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 73 cost = 0.138957492 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 74 cost = 0.139958592 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 75 cost = 0.139677357 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 76 cost = 0.138884079 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 77 cost = 0.139219756 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 78 cost = 0.138631248 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 79 cost = 0.136854943 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 80 cost = 0.137385232 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 81 cost = 0.136075481 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 82 cost = 0.136947306 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 83 cost = 0.136181633 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 84 cost = 0.135178527 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 85 cost = 0.136321018 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 86 cost = 0.135495113 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 87 cost = 0.135860966 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 88 cost = 0.134300424 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 89 cost = 0.133713768 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 90 cost = 0.133897732 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 91 cost = 0.133692303 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 92 cost = 0.134604666 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 93 cost = 0.133276543 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 94 cost = 0.133285560 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 95 cost = 0.132850133 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 96 cost = 0.131912743 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 97 cost = 0.132175505 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 98 cost = 0.132609605 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 99 cost = 0.132640077 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 100 cost = 0.131942113 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 101 cost = 0.132054462 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 102 cost = 0.131130034 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 103 cost = 0.130759505 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 104 cost = 0.131600313 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 105 cost = 0.131327772 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 106 cost = 0.130751184 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 107 cost = 0.130679788 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 108 cost = 0.130002338 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 109 cost = 0.129804671 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 110 cost = 0.129849577 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 111 cost = 0.129939320 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 112 cost = 0.129703679 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 113 cost = 0.129753352 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 114 cost = 0.129726440 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 115 cost = 0.131159695 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 116 cost = 0.128925728 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 117 cost = 0.129043707 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 118 cost = 0.128530224 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 119 cost = 0.128007738 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 120 cost = 0.128553620 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 121 cost = 0.128963887 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 122 cost = 0.128698226 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 123 cost = 0.126946221 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 124 cost = 0.128301615 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 125 cost = 0.127706056 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 126 cost = 0.127915293 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 127 cost = 0.128504509 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 128 cost = 0.127886997 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 129 cost = 0.127404292 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 130 cost = 0.127014880 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 131 cost = 0.126919868 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 132 cost = 0.127409949 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 133 cost = 0.128195439 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 134 cost = 0.126834927 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 135 cost = 0.125991449 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 136 cost = 0.125759285 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 137 cost = 0.126273803 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 138 cost = 0.126347243 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 139 cost = 0.126365420 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 140 cost = 0.125533435 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 141 cost = 0.125930785 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 142 cost = 0.126643535 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 143 cost = 0.126667799 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 144 cost = 0.127036180 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 145 cost = 0.126259684 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 146 cost = 0.125875002 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 147 cost = 0.126096834 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 148 cost = 0.125034885 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 149 cost = 0.124746935 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 150 cost = 0.125755428 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 151 cost = 0.124803588 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 152 cost = 0.125065559 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 153 cost = 0.124464219 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 154 cost = 0.124940449 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 155 cost = 0.124982881 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 156 cost = 0.125387387 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 157 cost = 0.124082232 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 158 cost = 0.123868915 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 159 cost = 0.125133927 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 160 cost = 0.125956495 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 161 cost = 0.125311873 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 162 cost = 0.124770553 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 163 cost = 0.123936541 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 164 cost = 0.124503680 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 165 cost = 0.124423693 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 166 cost = 0.124774081 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 167 cost = 0.122973431 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 168 cost = 0.124222090 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 169 cost = 0.123934777 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 170 cost = 0.123555405 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 171 cost = 0.123952377 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 172 cost = 0.124306157 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 173 cost = 0.124230850 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 174 cost = 0.124229193 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 175 cost = 0.123275107 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 176 cost = 0.123625344 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 177 cost = 0.123919601 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 178 cost = 0.124072452 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 179 cost = 0.123630443 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 180 cost = 0.123591777 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 181 cost = 0.122651705 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 182 cost = 0.125054576 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 183 cost = 0.123725061 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 184 cost = 0.122958092 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 185 cost = 0.123070417 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 186 cost = 0.123080645 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 187 cost = 0.123093547 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 188 cost = 0.123193937 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 189 cost = 0.121619677 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 190 cost = 0.123486568 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 191 cost = 0.122799935 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 192 cost = 0.123281320 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 193 cost = 0.123136991 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 194 cost = 0.124213465 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 195 cost = 0.122924512 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 196 cost = 0.123565792 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 197 cost = 0.122534154 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 198 cost = 0.123245996 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 199 cost = 0.123157377 Training accuracy: 1.0  Training auc: 1.0\n",
            "Epoch: 200 cost = 0.122397212 Training accuracy: 1.0  Training auc: 1.0\n",
            "*****===== Testing accuracy:  0.953405  Testing auc:  0.9940230384698978 =====*****\n",
            "0.9940230384698978\n",
            "0.9939687024559878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L21OSbzGlewx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "values_train = values[:700, :]\n",
        "values_test = values[700:, :]\n",
        "y_train = labels[:700]\n",
        "y_test = labels[700:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERl12RDFfZT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3aaeb970-542d-4cac-8488-4e056c41d28c"
      },
      "source": [
        "#RF with 1000 trees\n",
        "rf2 = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
        "rf2.fit(values_train, y_train)\n",
        "acc_rf2 = 1 - sum(abs(rf2.predict(values_test)-y_test))/len(y_test)\n",
        "auc_rf2 = metrics.roc_auc_score(y_test, rf2.predict(values_test))\n",
        "auc_rf2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.952754835905238"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEVzS_a2f4pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77369144-1efc-438c-c4b3-723aa341c2b8"
      },
      "source": [
        "#SVM with Sigmoid kernel\n",
        "clf = svm.SVC()\n",
        "clf.fit(values_train, y_train)\n",
        "acc_clf = 1 - sum(abs(clf.predict(values_test)-y_test))/len(y_test)\n",
        "auc_clf = metrics.roc_auc_score(y_test, clf.predict(values_test))\n",
        "print(auc_clf)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9772603781786569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z4-I-YhghXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9da8f833-e2b0-4bcd-c4f8-c7f0c1c6fd75"
      },
      "source": [
        "#linear SVM with full features\n",
        "linear_clf = svm.LinearSVC()\n",
        "linear_clf.fit(values_train,y_train)\n",
        "acc_linear_clf = 1 - sum(abs(linear_clf.predict(values_test)-y_test))/len(y_test)\n",
        "auc_linear_clf = metrics.roc_auc_score(y_test, linear_clf.predict(values_test))\n",
        "print(auc_linear_clf)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.974978265594436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGc6Rhf0sobw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "cd1aa424-a426-4c74-8804-15a2c2667eb5"
      },
      "source": [
        "#detect important features\n",
        "def plot_coefficients(classifier, top_features=200):\n",
        " coef = classifier.coef_.ravel()\n",
        " top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
        " top_negative_coefficients = np.argsort(coef)[:top_features]\n",
        " top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
        "\n",
        " plt.figure(figsize=(15, 5))\n",
        " colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
        " plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
        " plt.show()\n",
        " return(top_coefficients) \n",
        "\n",
        "\n",
        "top_features = plot_coefficients(linear_clf)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_unzJAyGgwm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Selection\n",
        "compressed = initial.iloc[:,top_features]\n",
        "values = compressed.values\n",
        "values, labels = shuffle(values, labels,random_state=1) \n",
        "values_train2 = values[:700, :]\n",
        "values_test2 = values[700:, :]\n",
        "y_train2 = labels[:700]\n",
        "y_test2 = labels[700:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnW0UM2GmKtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dcfcda47-9d47-4156-c88b-b87cde6176b1"
      },
      "source": [
        "#linear SVM with selected features\n",
        "linear_clf2 = svm.LinearSVC()\n",
        "linear_clf2.fit(values_train2,y_train2)\n",
        "acc_linear_clf2 = 1 - sum(abs(linear_clf2.predict(values_test2)-y_test2))/len(y_test2)\n",
        "auc_linear_clf2 = metrics.roc_auc_score(y_test2, linear_clf2.predict(values_test2))\n",
        "print(auc_linear_clf2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4957726109689621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffSzTbIhP_Q4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9c2fc7e9-6aac-4db4-f58e-318d38698886"
      },
      "source": [
        "#comparison\n",
        "RF_auc = pd.DataFrame({'RF with 500 Tress':[auc_rf],'RF with 1000 Tress':[auc_rf2]})\n",
        "fdnn_svd_auc = pd.DataFrame({'fDNN':[auc],'Sigmoid SVM':[auc_clf],'Full Linear SVM':[auc_linear_clf],'FS Linear SVM':[auc_linear_clf2]})\n",
        "print(RF_auc)\n",
        "print(fdnn_svd_auc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   RF with 500 Tress  RF with 1000 Tress\n",
            "0           0.993969            0.952755\n",
            "       fDNN  Sigmoid SVM  Full Linear SVM  FS Linear SVM\n",
            "0  0.994023      0.97726         0.974978       0.495773\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}